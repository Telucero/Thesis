{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm==4.62.1\n",
      "  Downloading tqdm-4.62.1-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.49.0\n",
      "    Uninstalling tqdm-4.49.0:\n",
      "      Successfully uninstalled tqdm-4.49.0\n",
      "Successfully installed tqdm-4.62.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm==4.62.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tqdm 2.0.0\n",
      "Uninstalling tqdm-2.0.0:\n",
      "  Successfully uninstalled tqdm-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7fa6b2e85790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py\", line 1122, in __del__\n",
      "  File \"/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (4.30.2)\n",
      "Requirement already satisfied: huggingface_hub in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (0.15.1)\n",
      "Requirement already satisfied: filelock in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: requests in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: requests in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp38-cp38-macosx_10_11_x86_64.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Installing collected packages: tokenizers, safetensors, transformers\n",
      "Successfully installed safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.23.5)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-macosx_10_14_x86_64.whl (24.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.24.0)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-macosx_10_9_x86_64.whl (359 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.3/359.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from datasets) (20.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets)\n",
      "  Downloading charset_normalizer-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp38-cp38-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp38-cp38-macosx_10_9_x86_64.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Collecting packaging (from datasets)\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: xxhash, tqdm, pyarrow, packaging, multidict, fsspec, frozenlist, dill, charset-normalizer, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.50.2\n",
      "    Uninstalling tqdm-4.50.2:\n",
      "      Successfully uninstalled tqdm-4.50.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.8.3\n",
      "    Uninstalling fsspec-0.8.3:\n",
      "      Successfully uninstalled fsspec-0.8.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 4.1.5 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "spyder 4.1.5 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 datasets-2.13.1 dill-0.3.6 frozenlist-1.3.3 fsspec-2023.6.0 huggingface-hub-0.15.1 multidict-6.0.4 multiprocess-0.70.14 packaging-23.1 pyarrow-12.0.1 tqdm-4.65.0 xxhash-3.2.0 yarl-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: portalocker\n",
      "Successfully installed portalocker-2.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "#import torchtext.datasets as datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from torchtext.datasets import MultiNLI\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNLI dataset can be used to explore decision boundaries and \n",
    "# interpretability in natural language inference tasks. By analyzing the decision \n",
    "# boundaries of models trained on this dataset, you can gain insights into how models \n",
    "# make predictions and understand the relationship between input sentences. \n",
    "#The dataset's diverse genres provide a good opportunity to examine how decision boundaries \n",
    "# vary across different types of text. Additionally, feature selection techniques can be applied to \n",
    "# investigate which features contribute most significantly to the decision boundaries and prediction outcomes.\n",
    "\n",
    "# Training \n",
    "\n",
    "# Evaluation is done using the dev_matched and testing dev_mismatched "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the field size limit\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as jsonl_file:\n",
    "        data = [json.loads(line) for line in jsonl_file]\n",
    "    return data\n",
    "\n",
    "def read_mnli_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file, delimiter='\\t')\n",
    "        dataset = [row for row in reader]\n",
    "    return dataset\n",
    "\n",
    "def convert_to_dataframe(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df = df[['sentence1', 'sentence2', 'gold_label']]  # Could include Genre,promptID, and PairID\n",
    "    return df\n",
    "\n",
    "# Path to the downloaded JSONL and TXT files\n",
    "train_file = os.path.expanduser(\"~/Desktop/Projects/Thesis/MNLI/multinli_1.0_train.txt\")\n",
    "\n",
    "validation_matched_file_txt = os.path.expanduser('~/Desktop/Projects/Thesis/MNLI/multinli_1.0_dev_mismatched.txt')\n",
    "validation_matched_file_jsonl = os.path.expanduser('~/Desktop/Projects/Thesis/MNLI/multinli_1.0_dev_matched.jsonl')\n",
    "\n",
    "test_matched_file_txt = os.path.expanduser('~/Desktop/Projects/Thesis/MNLI/multinli_1.0_dev_mismatched.jsonl')\n",
    "test_matched_file_jsonl = os.path.expanduser('~/Desktop/Projects/Thesis/MNLI/multinli_1.0_dev_mismatched.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnli_dataset = read_mnli_dataset(train_file)\n",
    "mnli_train_df = convert_to_dataframe(mnli_dataset)\n",
    "\n",
    "validation_dataset = read_mnli_dataset(validation_matched_file_txt)\n",
    "validation_df = convert_to_dataframe(validation_dataset)\n",
    "\n",
    "\n",
    "# Doesn't follow the same format as the validation \n",
    "#test_dataset = read_mnli_dataset(test_matched_file_txt)\n",
    "#test_df = convert_to_dataframe(test_dataset)\n",
    "#test_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391171</th>\n",
       "      <td>Clearly, California can - and must - do better.</td>\n",
       "      <td>California cannot do any better.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391172</th>\n",
       "      <td>It was once regarded as the most beautiful str...</td>\n",
       "      <td>So many of the original buildings had been rep...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391173</th>\n",
       "      <td>Houseboats are a beautifully preserved traditi...</td>\n",
       "      <td>The tradition of houseboats originated while t...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391174</th>\n",
       "      <td>Obituaries fondly recalled his on-air debates ...</td>\n",
       "      <td>The obituaries were beautiful and written in k...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391175</th>\n",
       "      <td>in that other you know uh that i should do it ...</td>\n",
       "      <td>My husband has been so overworked lately that ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391176 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "0       Conceptually cream skimming has two basic dime...   \n",
       "1       you know during the season and i guess at at y...   \n",
       "2       One of our number will carry out your instruct...   \n",
       "3       How do you know? All this is their information...   \n",
       "4       yeah i tell you what though if you go price so...   \n",
       "...                                                   ...   \n",
       "391171    Clearly, California can - and must - do better.   \n",
       "391172  It was once regarded as the most beautiful str...   \n",
       "391173  Houseboats are a beautifully preserved traditi...   \n",
       "391174  Obituaries fondly recalled his on-air debates ...   \n",
       "391175  in that other you know uh that i should do it ...   \n",
       "\n",
       "                                                sentence2     gold_label  \n",
       "0       Product and geography are what make cream skim...        neutral  \n",
       "1       You lose the things to the following level if ...     entailment  \n",
       "2       A member of my team will execute your orders w...     entailment  \n",
       "3                       This information belongs to them.     entailment  \n",
       "4                The tennis shoes have a range of prices.        neutral  \n",
       "...                                                   ...            ...  \n",
       "391171                   California cannot do any better.  contradiction  \n",
       "391172  So many of the original buildings had been rep...        neutral  \n",
       "391173  The tradition of houseboats originated while t...     entailment  \n",
       "391174  The obituaries were beautiful and written in k...        neutral  \n",
       "391175  My husband has been so overworked lately that ...        neutral  \n",
       "\n",
       "[391176 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your contribution helped make it possible for ...</td>\n",
       "      <td>Your contributions were of no help with our st...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The answer has nothing to do with their cause,...</td>\n",
       "      <td>Dictionaries are indeed exercises in bi-unique...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We serve a classic Tuscan meal that includes ...</td>\n",
       "      <td>We serve a meal of Florentine terrine.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A few months ago, Carl Newton and I wrote a le...</td>\n",
       "      <td>Carl Newton and I have never had any other pre...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was on this earth you know, I've lived on th...</td>\n",
       "      <td>I don't yet know the reason why I have lived o...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Do you watch that?</td>\n",
       "      <td>Can you see?</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>To a Western ear, the most predictable of lang...</td>\n",
       "      <td>To the Western ear, the least predictable of l...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>The recorder captured the sounds of loud thump...</td>\n",
       "      <td>The recorder didn't capture any of the sounds.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>That's a good attitude!</td>\n",
       "      <td>You feel good about this, don't you?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Bloomer (for `flower'), butter (for `ram'), or...</td>\n",
       "      <td>Bloomer is another word for flower, butter is ...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence1  \\\n",
       "0     Your contribution helped make it possible for ...   \n",
       "1     The answer has nothing to do with their cause,...   \n",
       "2      We serve a classic Tuscan meal that includes ...   \n",
       "3     A few months ago, Carl Newton and I wrote a le...   \n",
       "4     I was on this earth you know, I've lived on th...   \n",
       "...                                                 ...   \n",
       "9995                                 Do you watch that?   \n",
       "9996  To a Western ear, the most predictable of lang...   \n",
       "9997  The recorder captured the sounds of loud thump...   \n",
       "9998                            That's a good attitude!   \n",
       "9999  Bloomer (for `flower'), butter (for `ram'), or...   \n",
       "\n",
       "                                              sentence2     gold_label  \n",
       "0     Your contributions were of no help with our st...  contradiction  \n",
       "1     Dictionaries are indeed exercises in bi-unique...  contradiction  \n",
       "2                We serve a meal of Florentine terrine.     entailment  \n",
       "3     Carl Newton and I have never had any other pre...  contradiction  \n",
       "4     I don't yet know the reason why I have lived o...     entailment  \n",
       "...                                                 ...            ...  \n",
       "9995                                       Can you see?  contradiction  \n",
       "9996  To the Western ear, the least predictable of l...  contradiction  \n",
       "9997     The recorder didn't capture any of the sounds.  contradiction  \n",
       "9998              You feel good about this, don't you?         neutral  \n",
       "9999  Bloomer is another word for flower, butter is ...     entailment  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " entailment       130417\n",
       " contradiction    130381\n",
       " neutral          130378\n",
       " Name: gold_label, dtype: int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mnli_df['gold_label'].nunique(), mnli_df['gold_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " entailment       3463\n",
       " contradiction    3240\n",
       " neutral          3129\n",
       " -                 168\n",
       " Name: gold_label, dtype: int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "validation_df['gold_label'].nunique(), validation_df['gold_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "# The root needs to be changed but idk exactly what it does or what it entails\n",
    "\n",
    "train_data, dev_matched_data, dev_mismatched_data = datasets.MNLI(root='MNLI', split=('train', 'dev_matched', 'dev_mismatched'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Print some examples from the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7fa6b2e85790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py\", line 1122, in __del__\n",
      "    if self.total is not None:\n",
      "  File \"/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='success', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7fa6b2e85790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py\", line 1122, in __del__\n",
      "    if self.total is not None:\n",
      "  File \"/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='success', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Some weights of the model checkpoint at ./MNLI/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./MNLI/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Use a GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('./MNLI/bert-base-uncased', do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained('./MNLI/bert-base-uncased', \n",
    "    num_labels = 3, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    # Tokenize and prepare input data\n",
    "    sentences = df[['sentence1', 'sentence2']].values.tolist()\n",
    "    labels = df['gold_label'].apply(lambda label: 0 if label=='entailment' else 1 if label=='neutral' else 2).values\n",
    "\n",
    "    # Create attention masks\n",
    "    input_ids, attention_masks = [], []\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                     \n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 128,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0), torch.tensor(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/taylorlucero/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks, train_labels = prepare_dataset(mnli_train_df)\n",
    "validation_inputs, validation_masks, validation_labels = prepare_dataset(validation_df)\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )\n",
    "\n",
    "# Now you would move onto defining your training loop and start training your model.\n",
    "# As this process can be quite involved, I'll stop here.\n",
    "# Make sure to also evaluate your model on the validation data to check its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Assume features and labels are your data\\nfeatures, labels = torch.randn(100, 10), torch.randint(0, 2, (100,))\\n\\n# Convert your data into a Dataset\\ndataset = SimpleDataset(features, labels)\\n\\n# Split dataset into training and validation\\n#train_size = int(0.8 * len(dataset))\\n#val_size = len(dataset) - train_size\\n#train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\\n\\n# Create DataLoaders for training and validation\\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\\nval_dataloader = DataLoader(val_dataset, batch_size=16'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Let's create a simple dataset\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\"\"\"# Assume features and labels are your data\n",
    "features, labels = torch.randn(100, 10), torch.randint(0, 2, (100,))\n",
    "\n",
    "# Convert your data into a Dataset\n",
    "dataset = SimpleDataset(features, labels)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#val_size = len(dataset) - train_size\n",
    "#train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# We'll use number of epochs and the number of training steps as the argument to the scheduler function\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Specify the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{epochs}')\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Move the batch to the device\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Clear any previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        # Get the loss from the outputs tuple: (loss, logits)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        # Gradient clipping helps prevent \"exploding gradients\" problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Average loss\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Train loss: {avg_train_loss}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in validation_dataloader:\n",
    "        # Move the batch to the device\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        # Get the logits from the outputs tuple: (loss, logits)\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "        total_loss += loss_fn(torch.tensor(logits), torch.tensor(label_ids)).item()\n",
    "\n",
    "    # Average loss\n",
    "    avg_val_loss = total_loss / len(validation_dataloader)\n",
    "    print(f'Validation loss: {avg_val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function to grab the activation values in the forward pass\n",
    "\n",
    "activations = []\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "hook_handle = model.bert.embeddings.register_forward_hook(forward_hook)\n",
    "\n",
    "# Run your model\n",
    "\n",
    "hook_handle.remove()\n",
    "\n",
    "# Now `activations` contains the outputs of the module you hooked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_feature_importance(model, X, y, n_iterations=100):\n",
    "    feature_importances = np.zeros(X.shape[1])\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        X_perturbed = X.copy()  # Make a copy of the original features\n",
    "        \n",
    "        # Perturb the feature values by adding random noise\n",
    "        for j in range(X.shape[1]):\n",
    "            noise = np.random.normal(0, 1, X.shape[0])  # Generate random noise\n",
    "            X_perturbed[:, j] += noise\n",
    "        \n",
    "        # Make predictions with the perturbed features\n",
    "        y_pred = model.predict(X_perturbed)\n",
    "        \n",
    "        # Measure the impact on performance (e.g., accuracy)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        # Accumulate the impact scores for each feature\n",
    "        feature_importances += (accuracy - model.score(X, y)) / n_iterations\n",
    "    \n",
    "    return feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_feature_importance(model, X, y, n_iterations=100):\n",
    "    feature_importances = np.zeros(X.shape[1])\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        X_shuffled, y_shuffled = shuffle(X, y)  # Randomly shuffle the data\n",
    "        \n",
    "        # Make predictions with the shuffled features\n",
    "        y_pred = model.predict(X_shuffled)\n",
    "        \n",
    "        # Measure the impact on performance (e.g., accuracy)\n",
    "        accuracy = accuracy_score(y_shuffled, y_pred)\n",
    "        \n",
    "        # Accumulate the impact scores for each feature\n",
    "        feature_importances += (accuracy - model.score(X, y)) / n_iterations\n",
    "    \n",
    "    return feature_importances\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
